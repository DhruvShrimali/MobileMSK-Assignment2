{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad68e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "# from diffusion_utilities import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6008ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a14ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3afc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe65170",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResidualConvBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# construct model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m \u001b[43mContextUnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cfeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mContextUnet.__init__\u001b[0;34m(self, in_channels, n_feat, n_cfeat, height)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m height  \u001b[38;5;66;03m#assume h == w. must be divisible by 4, so 28,24,20,16...\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the initial convolutional layer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv \u001b[38;5;241m=\u001b[39m \u001b[43mResidualConvBlock\u001b[49m(in_channels, n_feat, is_res\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the down-sampling path of the U-Net with two levels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1 \u001b[38;5;241m=\u001b[39m UnetDown(n_feat, n_feat)        \u001b[38;5;66;03m# down1 #[10, 256, 8, 8]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResidualConvBlock' is not defined"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b597136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb0e3162",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_trained.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_trained.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef48f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405db379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize samples\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m----> 3\u001b[0m samples, intermediate_ddpm \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m animation_ddpm \u001b[38;5;241m=\u001b[39m plot_sample(intermediate_ddpm,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m HTML(animation_ddpm\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36msample_ddpm\u001b[0;34m(n_sample, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73219b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrectly sample without adding in noise\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_incorrect(n_sample):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # don't add back in noise\n",
    "        z = 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i%20==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8bb978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize samples\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m----> 3\u001b[0m samples, intermediate \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm_incorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m animation \u001b[38;5;241m=\u001b[39m plot_sample(intermediate,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m HTML(animation\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36msample_ddpm_incorrect\u001b[0;34m(n_sample)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate = sample_ddpm_incorrect(32)\n",
    "animation = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8dc7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "616cc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ebe17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fedfa065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000c53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                        \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8708f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a38e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6dfd2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResidualConvBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# construct model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m \u001b[43mContextUnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cfeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mContextUnet.__init__\u001b[0;34m(self, in_channels, n_feat, n_cfeat, height)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m height  \u001b[38;5;66;03m#assume h == w. must be divisible by 4, so 28,24,20,16...\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the initial convolutional layer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv \u001b[38;5;241m=\u001b[39m \u001b[43mResidualConvBlock\u001b[49m(in_channels, n_feat, is_res\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the down-sampling path of the U-Net with two levels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1 \u001b[38;5;241m=\u001b[39m UnetDown(n_feat, n_feat)        \u001b[38;5;66;03m# down1 #[10, 256, 8, 8]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResidualConvBlock' is not defined"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d5202f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load dataset and construct optimizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./sprites_1788_16x16.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./sprite_labels_nc_1788_16x16.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform, null_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(nn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlrate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset(\"./sprites_1788_16x16.npy\", \"./sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f2ceecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de0a977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22e27ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd31ace1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_0.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_0.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed4b675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize samples\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m----> 3\u001b[0m samples, intermediate_ddpm \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m animation_ddpm \u001b[38;5;241m=\u001b[39m plot_sample(intermediate_ddpm,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m HTML(animation_ddpm\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m, in \u001b[0;36msample_ddpm\u001b[0;34m(n_sample, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f3bfbb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_4.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_4.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f48d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize samples\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m----> 3\u001b[0m samples, intermediate_ddpm \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m animation_ddpm \u001b[38;5;241m=\u001b[39m plot_sample(intermediate_ddpm,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m HTML(animation_ddpm\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m, in \u001b[0;36msample_ddpm\u001b[0;34m(n_sample, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf1244b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_8.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_8.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59ba1440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize samples\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m----> 3\u001b[0m samples, intermediate_ddpm \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m animation_ddpm \u001b[38;5;241m=\u001b[39m plot_sample(intermediate_ddpm,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m HTML(animation_ddpm\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m, in \u001b[0;36msample_ddpm\u001b[0;34m(n_sample, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bc77b16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_31.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_31.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bab4307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize samples\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m----> 3\u001b[0m samples, intermediate_ddpm \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m animation_ddpm \u001b[38;5;241m=\u001b[39m plot_sample(intermediate_ddpm,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m HTML(animation_ddpm\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m, in \u001b[0;36msample_ddpm\u001b[0;34m(n_sample, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fde9f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample  \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                        \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f0a7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d06d9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44699697",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResidualConvBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# construct model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m \u001b[43mContextUnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cfeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m, in \u001b[0;36mContextUnet.__init__\u001b[0;34m(self, in_channels, n_feat, n_cfeat, height)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m height  \u001b[38;5;66;03m#assume h == w. must be divisible by 4, so 28,24,20,16...\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the initial convolutional layer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv \u001b[38;5;241m=\u001b[39m \u001b[43mResidualConvBlock\u001b[49m(in_channels, n_feat, is_res\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the down-sampling path of the U-Net with two levels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1 \u001b[38;5;241m=\u001b[39m UnetDown(n_feat, n_feat)        \u001b[38;5;66;03m# down1 #[10, 256, 8, 8]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResidualConvBlock' is not defined"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a717b01b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResidualConvBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# reset neural network\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m \u001b[43mContextUnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cfeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# re setup optimizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(nn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlrate)\n",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m, in \u001b[0;36mContextUnet.__init__\u001b[0;34m(self, in_channels, n_feat, n_cfeat, height)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m height  \u001b[38;5;66;03m#assume h == w. must be divisible by 4, so 28,24,20,16...\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the initial convolutional layer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv \u001b[38;5;241m=\u001b[39m \u001b[43mResidualConvBlock\u001b[49m(in_channels, n_feat, is_res\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the down-sampling path of the U-Net with two levels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1 \u001b[38;5;241m=\u001b[39m UnetDown(n_feat, n_feat)        \u001b[38;5;66;03m# down1 #[10, 256, 8, 8]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResidualConvBlock' is not defined"
     ]
    }
   ],
   "source": [
    "# reset neural network\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
    "\n",
    "# re setup optimizer\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf758222",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in pretrain model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/context_model_trained.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Context Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in pretrain model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_trained.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "print(\"Loaded in Context Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b53a7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51a77be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample with context using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_context(n_sample, context, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t, ctx)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75cf4103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[1;32m      3\u001b[0m ctx \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, (\u001b[38;5;241m32\u001b[39m,)), \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 4\u001b[0m samples, intermediate \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m animation_ddpm_context \u001b[38;5;241m=\u001b[39m plot_sample(intermediate,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m HTML(animation_ddpm_context\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 18\u001b[0m, in \u001b[0;36msample_ddpm_context\u001b[0;34m(n_sample, context, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t, c\u001b[38;5;241m=\u001b[39mcontext)    \u001b[38;5;66;03m# predict noise e_(x_t,t, ctx)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples with randomly selected context\n",
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddpm_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7879053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, nrow=2):\n",
    "    _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4,2 ))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax in zip(imgs, axs):\n",
    "        img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd03b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# user defined context\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# hero, non-hero, food, spell, side-facing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     12\u001b[0m ])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m show_images(samples)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 18\u001b[0m, in \u001b[0;36msample_ddpm_context\u001b[0;34m(n_sample, context, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t, c\u001b[38;5;241m=\u001b[39mcontext)    \u001b[38;5;66;03m# predict noise e_(x_t,t, ctx)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# user defined context\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0],  \n",
    "    [1,0,0,0,0],    \n",
    "    [0,0,0,0,1],\n",
    "    [0,0,0,0,1],    \n",
    "    [0,1,0,0,0],\n",
    "    [0,1,0,0,0],\n",
    "    [0,0,1,0,0],\n",
    "    [0,0,1,0,0],\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35051fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mix of defined context\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# hero, non-hero, food, spell, side-facing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],      \u001b[38;5;66;03m#human\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m ])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m show_images(samples)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 18\u001b[0m, in \u001b[0;36msample_ddpm_context\u001b[0;34m(n_sample, context, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# sample some random noise to inject back in. For i = 1, don't add back in noise\u001b[39;00m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t, c\u001b[38;5;241m=\u001b[39mcontext)    \u001b[38;5;66;03m# predict noise e_(x_t,t, ctx)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_add_noise(samples, i, eps, z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# mix of defined context\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0],      #human\n",
    "    [1,0,0.6,0,0],    \n",
    "    [0,0,0.6,0.4,0],  \n",
    "    [1,0,0,0,1],  \n",
    "    [1,1,0,0,0],\n",
    "    [1,0,0,1,0]\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2423eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a948568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), \n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize                       \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "        \n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "738085cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e43f2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29ded008",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResidualConvBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# construct model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m \u001b[43mContextUnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cfeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[46], line 12\u001b[0m, in \u001b[0;36mContextUnet.__init__\u001b[0;34m(self, in_channels, n_feat, n_cfeat, height)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m height  \u001b[38;5;66;03m#assume h == w. must be divisible by 4, so 28,24,20,16...\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the initial convolutional layer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv \u001b[38;5;241m=\u001b[39m \u001b[43mResidualConvBlock\u001b[49m(in_channels, n_feat, is_res\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the down-sampling path of the U-Net with two levels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1 \u001b[38;5;241m=\u001b[39m UnetDown(n_feat, n_feat)        \u001b[38;5;66;03m# down1 #[10, 256, 8, 8]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResidualConvBlock' is not defined"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11d72e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sampling function for DDIM   \n",
    "# removes the noise using ddim\n",
    "def denoise_ddim(x, t, t_prev, pred_noise):\n",
    "    ab = ab_t[t]\n",
    "    ab_prev = ab_t[t_prev]\n",
    "    \n",
    "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
    "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
    "\n",
    "    return x0_pred + dir_xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7910c37b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_31.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Model without context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_31.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "print(\"Loaded in Model without context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86b7a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample quickly using DDIM\n",
    "@torch.no_grad()\n",
    "def sample_ddim(n_sample, n=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7d877f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize samples\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m----> 3\u001b[0m samples, intermediate \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m animation_ddim \u001b[38;5;241m=\u001b[39m plot_sample(intermediate,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m HTML(animation_ddim\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 16\u001b[0m, in \u001b[0;36msample_ddim\u001b[0;34m(n_sample, n)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# reshape time tensor\u001b[39;00m\n\u001b[1;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;241m/\u001b[39m timesteps])[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_ddim(samples, i, i \u001b[38;5;241m-\u001b[39m step_size, eps)\n\u001b[1;32m     18\u001b[0m intermediate\u001b[38;5;241m.\u001b[39mappend(samples\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate = sample_ddim(32, n=25)\n",
    "animation_ddim = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49806c3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in model weights and set to eval mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/context_model_31.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m nn_model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in Context Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_31.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "print(\"Loaded in Context Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a6c3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast sampling algorithm with context\n",
    "@torch.no_grad()\n",
    "def sample_ddim_context(n_sample, context, n=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "874e268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[1;32m      3\u001b[0m ctx \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, (\u001b[38;5;241m32\u001b[39m,)), \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 4\u001b[0m samples, intermediate \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddim_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m animation_ddpm_context \u001b[38;5;241m=\u001b[39m plot_sample(intermediate,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m4\u001b[39m,save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mani_run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m HTML(animation_ddpm_context\u001b[38;5;241m.\u001b[39mto_jshtml())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 16\u001b[0m, in \u001b[0;36msample_ddim_context\u001b[0;34m(n_sample, context, n)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# reshape time tensor\u001b[39;00m\n\u001b[1;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;241m/\u001b[39m timesteps])[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t, c\u001b[38;5;241m=\u001b[39mcontext)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_ddim(samples, i, i \u001b[38;5;241m-\u001b[39m step_size, eps)\n\u001b[1;32m     18\u001b[0m intermediate\u001b[38;5;241m.\u001b[39mappend(samples\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddim_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7bc411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39b3db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "572cd6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 500\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-r 1 sample_ddim(32, n=25)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r 1 sample_ddpm(32, )\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2369\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2369\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/magics/execution.py:1163\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m   1162\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[0;32m-> 1163\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/magics/execution.py:157\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    155\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 16\u001b[0m, in \u001b[0;36msample_ddim\u001b[0;34m(n_sample, n)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# reshape time tensor\u001b[39;00m\n\u001b[1;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;241m/\u001b[39m timesteps])[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m(samples, t)    \u001b[38;5;66;03m# predict noise e_(x_t,t)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m samples \u001b[38;5;241m=\u001b[39m denoise_ddim(samples, i, i \u001b[38;5;241m-\u001b[39m step_size, eps)\n\u001b[1;32m     18\u001b[0m intermediate\u001b[38;5;241m.\u001b[39mappend(samples\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "%timeit -r 1 sample_ddim(32, n=25)\n",
    "%timeit -r 1 sample_ddpm(32, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d8a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
